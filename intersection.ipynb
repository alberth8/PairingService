{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking ingredients\n",
    "\n",
    "Similar to the suggested pairings problem I solved with Dijkstra's algorithm, I had to find a way to solve a mundane problem that had a very clear naive solution. The problem was stated as so:\n",
    "\n",
    "> Given a set of ingredients, find all ingredients that are commonly paired with the given set and rank them.\n",
    "\n",
    "Assuming this data is stored in a relational database, such as MySQL, and depending on the schema design, it is not difficult to see that this would be a sequence of `INNER JOIN`s. As it turned out, the way we designed our schema for Saffron made it slightly trickier, but it was still very much do-able with the use of a junction table. To the address the ranking, we could rank based on total number of times that an ingredient $x$ showed up.\n",
    "\n",
    "...But what's the fun in that? So I took it upon myself to do it in Python, a language I wasn't familiar with, and to find a more interesting method to rank the ingredients.\n",
    "\n",
    "Side note: $\\LaTeX$ doesn't seem to display properly on github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjacency Matrix\n",
    "\n",
    "Let's take a look at simple example. In our actual adjacency matrix, columns and rows have the same labels, but for demonstration purposes, I've made them different. What the user gives us are the rows, and what we try to match those with are the columns.\n",
    "\n",
    "Suppose we have a matrix as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a1  b1  c1  d1\n",
      "a   1   2   0   3\n",
      "b   0   1   0   0\n",
      "c   1   0   1   1\n",
      "d   0   0   0   0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([[1, 2, 0, 3], [0, 1, 0, 0], [1, 0, 1, 1], [0, 0, 0, 0]], index=['a', 'b', 'c', 'd'], columns=['a1', 'b1', 'c1', 'd1'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're only interested in ingredients `a` and `c`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a1  b1  c1  d1\n",
      "a   1   2   0   3\n",
      "c   1   0   1   1\n"
     ]
    }
   ],
   "source": [
    "dftemp = df.loc[['a','c'], :]\n",
    "print(dftemp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take only the ingredients that have been paired with *all* of the ingredients the user has given us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a1  d1\n",
      "a   1   3\n",
      "c   1   1\n"
     ]
    }
   ],
   "source": [
    "print(dftemp.loc[:,dftemp.columns[(dftemp >= 1).all()]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now solved the first issue of finding the intersection: If total count was our ranking criteria, the above result would be `[d1, a1]`.  But suppose we had a set where two of ingredients had a total sum of 100, it's not as clear which one should be ranked higher.\n",
    "\n",
    "This is when I started looking at edge cases, and started thinking about the distribution of each column. It made sense to think that ingredients that would pair well would be ones that are often paired with all the one's that are being considered, as opposed to an ingredient that was only paired once with nine of say, 10, but 100 times with the tenth ingredient.\n",
    "\n",
    "My first approach was to consider a Pearson's $\\chi^2$ test for goodness of fit, but this only provides a test, as opposed to a measure. We also didn't have enough data to properly conduct such a test. So I sought to find a simple estimate to measure the uniformity of our observed data. Entropy seemed to be quite reasonable.\n",
    "\n",
    "$$ \n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathrm{H}(x) &= \\sum_{i=1}^n {\\mathrm{P}(x_i)\\,\\mathrm{I}(x_i)}\\\\\n",
    " &= -\\sum_{i=1}^n {\\mathrm{P}(x_i) \\log_b \\mathrm{P}(x_i)} \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "[Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory) measures the unpredictability of information content. And it's value is maximized in a [uniform distribution](https://en.wikipedia.org/wiki/Entropy_(information_theory)\n",
    "\n",
    "$$log_{10}(n), \\; n = b-a+1,$$\n",
    "\n",
    "where `b` is the upper bound, and `a` is the lower bound.\n",
    "\n",
    "We now have two quantities to take in to account, total count and entropy. One possibility would be to express the two as a linear combination, but I wanted to penalize heavily skewed distributions and reward those that were well distributed. Thus, I decided to take the sum of each ingredient and  raise each to it's entropy. This in effect heavily rewards uniformly distributed vectors, while penalizing skewed ones.\n",
    "\n",
    "Here's the code:\n",
    "\n",
    "    def find_intersection(clean_df, user_ingredients):\n",
    "        # Consider only the rows of the desired user_ingredients\n",
    "        df_temp = clean_df.loc[user_ingredients, :]\n",
    "\n",
    "        # Drop columns that match `user_ingredients`. Examining shape of df_temp\n",
    "        # before and after, column size decreases by 2\n",
    "        df_temp = df_temp.drop(labels=user_ingredients, axis=1)\n",
    "\n",
    "        # Then consider only the columns that have been paired with all ingredients of interest\n",
    "        new_cols = df_temp.columns[(df_temp > float(0)).all()]\n",
    "\n",
    "        # Remove rows that are same as column names\n",
    "        final_df = df_temp.loc[:, new_cols]\n",
    "\n",
    "        # Rank by examining uniformity of distribution along columns, \n",
    "        # penalizing for non-uniformity, while rewarding for uniformity\n",
    "        sums = final_df.sum()\n",
    "        entropy = stats.entropy(final_df, base=2)\n",
    "        rankings = np.power(sums, entropy)\n",
    "        rankings_sorted = rankings.sort_values()\n",
    "\n",
    "        # Format into list of tuples (<col_name>, <ranking>)\n",
    "        cols = rankings_sorted.index.values\n",
    "        rankings_list = list(zip(cols, rankings))\n",
    "\n",
    "        return rankings_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Please see the related functions for further details. Thanks for viewing :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
